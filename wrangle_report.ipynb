{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling WeRateDogs twitter data - overview\n",
    "\n",
    "\n",
    "       \n",
    "       \n",
    "       Christine Shuttleworth, 1st of October 2020\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id='intro1'></a>\n",
    "## Introduction \n",
    "\n",
    "WeRateDogs Twitter data contains some very entertaining data rating dogs and their images, that are being sent by their owners. The data avaiable comes from three sources: firstly a csv archive file which contains most of the data from  the twitter archive of WeRateDogs. Secondly, some extra data that was extracted via the Twitter API and dog image predictions from a neural network outcome tsv file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_csv1'></a>\n",
    "#### Load twitter_archive_enhanced.csv and learn about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ta = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were not real problems reading in the csv file. Looking at the data however, some very obvious structural problems and qualitiy problems became apparent pretty quickly. They are listed in the assessment section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='twitter_api1'></a>\n",
    "#### Request data from the twitter API and load it into a dataframe\n",
    "\n",
    "Some additional data like retweet count and favorite count, where not extracted with the twitter archive. These I extracted using the twitter API. This step was by far the most difficult. Setting up a Twitter developer account and getting log in credentials was the first step. Then I looked for a way to access the API without hard coding the credentials into the Jupyter notebook. There are several ways to do this, see below. I finally decided to use the dotenv method. I used two methods to extract the extra data: once writing only the wanted data to a csv file or secondly writing the all the data available through the API to a text file using JSON dump. Using the second method, turned out to be more useful, as I found out that I then had extra data to look at, which I did not realise I wanted at the start.\n",
    "\n",
    "https://developer.twitter.com/en/docs/labs/tweets-and-users/quick-start/get-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using .env file and python-dotenv to keep access token safe\n",
    "#pip install -U python-dotenv\n",
    "\n",
    "#import os\n",
    "#from pathlib import Path  # Python 3.6+ only\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "consumer_key = os.getenv(\"TWAPIKEY\")\n",
    "consumer_secret = os.getenv(\"TWAPISECRETKEY\")\n",
    "\n",
    "#use tweepy to access twitter API with OAuth2\n",
    "\n",
    "auth = tw.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "#Other option to store passkey safely:\n",
    "#1. could use a python .config file and the config library to store access token e.g. with wikiart API\n",
    "#response = requests.get(f'https://www.wikiart.org/en/Api/2/login?accessCode={cfg.twitter['api_key']}&secretCode={cfg.twitter['api_secret_key']')\n",
    "\n",
    "#2. secure storage of access details with yaml\n",
    "#import yaml\n",
    "\n",
    "#with open(\"config.yml\", 'r') as ymlfile:\n",
    "#    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "#print(cfg[api_creds'access_code'])\n",
    "#print(cfg[api_creds'secret_code'])\n",
    "\n",
    "#3.using magic command to access variables in .env\n",
    "#%env\n",
    "##Get, set, or list environment variables.\n",
    "\n",
    "##Usage:\n",
    "\n",
    "#%env: lists all environment variables/values \n",
    "#%env var: get value for var \n",
    "#%env var val: set value for var \n",
    "#%env var=val: set value for var \n",
    "#%env var=$val: set value for var, \n",
    "    \n",
    "##using python expansion if possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in dataset:2331\n"
     ]
    }
   ],
   "source": [
    "## Extracting all data available through the twitter archive using the twitter API and writing the returned JSON to a text file.\n",
    "def check_hashtag(tw_json):\n",
    "    try:\n",
    "        hashtags = tw_json['entities']['hashtags'][0]['text'] \n",
    "    except IndexError:\n",
    "        hashtags = np.nan\n",
    "    return hashtags\n",
    "\n",
    "def check_jpg_url(tw_json):\n",
    "    try:\n",
    "        jpg_url = tw_json['entities']['media'][0]['url'] \n",
    "    except KeyError:\n",
    "        jpg_url = np.nan\n",
    "    return jpg_url\n",
    "\n",
    "def check_expanded_url(tw_json):\n",
    "    try:\n",
    "        expanded_url = tw_json['extended_entities']['media'][0]['expanded_url']\n",
    "    except KeyError:\n",
    "        expanded_url = np.nan\n",
    "    return expanded_url   \n",
    "                                  \n",
    "j=0\n",
    "\n",
    "df_json = pd.DataFrame(columns = ['tweet_id', 'favorite_count', 'retweet_count','hashtags','jpg_url_json', 'expanded_url'])\n",
    "\n",
    "with open ('tweet_json.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tw_json = json.loads(line)\n",
    "        j+=1\n",
    "\n",
    "        hashtags = check_hashtag(tw_json)\n",
    "        jpg_url = check_jpg_url(tw_json)\n",
    "        expanded_url = check_expanded_url(tw_json)\n",
    "\n",
    "        df_json = df_json.append({'tweet_id':tw_json['id'], 'favorite_count':tw_json['favorite_count'], 'retweet_count':tw_json['retweet_count'], \\\n",
    "                                 'hashtags':hashtags,'jpg_url_json':jpg_url, 'expanded_url':expanded_url}, ignore_index=True)       \n",
    "\n",
    "print(f'Total lines in dataset:{j}')                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests1'></a>\n",
    "#### Request data from URL and load .tsv file into dataframe \n",
    "\n",
    "Finally, the results of a neural network prediction of all the dogs images in the twitter archive was ingested using the requests library. There was no problem there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "#response.content\n",
    "\n",
    "with open('image-predictions.tsv', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "df_pre = pd.read_csv('image-predictions.tsv', delimiter='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tsa_course]",
   "language": "python",
   "name": "conda-env-tsa_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
